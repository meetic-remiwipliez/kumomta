# Default values for kumomta.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

kumoConfigFiles: []

replicaCount: 1

image:
  repository: ghcr.io/kumocorp/kumomta
  pullPolicy: IfNotPresent
  tag: 2025.03.19-1d3f1f67

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

deployment:
  args:
    - --policy=/opt/kumomta/etc/policy/init.lua
    - --user=kumod
  command:
    - /opt/kumomta/sbin/kumod

env:
  # Enable debug logging for all KumoMTA components
  # Format: component=level,component=level
  # Available levels: trace, debug, info, warn, error
  # Available components: kumod, lua, http, smtp, queue, etc.
  - name: KUMOD_LOG
    value: "kumod=debug,lua=debug,http=debug,smtp=debug,queue=debug,egress=debug,dns=debug"
  # Enable sink mode by default for safe testing
  # All emails will be redirected to the sink service instead of being sent
  # Set to "false" to disable sink mode and send real emails
  - name: KUMOMTA_SINK_ENABLED
    value: "true"
  # Sink endpoint: adjust the service name and namespace according to your Helm release
  # Service name format: {{ release-name }}-kumomta-sink.{{ namespace }}.svc.cluster.local
  # Default uses the same namespace as the release (will be set by template if needed)
  - name: KUMOMTA_SINK_ENDPOINT
    value: "kumomta-sink.kumomta.svc.cluster.local"
  # Sink IP: ClusterIP of the sink service (use [IP] format to avoid DNS/MX resolution)
  # Get it with: kubectl get svc -n kumomta kumomta-sink -o jsonpath='{.spec.clusterIP}'
  # If not set, will try to use KUMOMTA_SINK_SERVICE_HOST (Kubernetes auto-injected env var)
  # Note: Using IP directly avoids DNS resolution issues in Kubernetes
  - name: KUMOMTA_SINK_IP
    value: "" # Will be auto-populated from KUMOMTA_SINK_SERVICE_HOST if available
  # StirTalk egress source IPs (optional)
  # If not set, KumoMTA will use the Pod IP (default behavior in Kubernetes)
  # See: https://kumomta.com/blog/moving-from-momentum
  # These correspond to Momentum bindings StirTalk3 and StirTalk5
  # Only set these if you have specific IPs configured at the network level
  - name: KUMOMTA_STIRTALK_3_IP
    value: ""
  - name: KUMOMTA_STIRTALK_5_IP
    value: ""
  # Redis/Dragonfly configuration for shared throttles
  # Using Dragonfly in namespace dragonflydb, service name dragonflydb
  # Format: redis://[username:password@]host:port[/database]
  # Dragonfly uses Redis protocol, default port is 6379
  - name: KUMOMTA_REDIS_HOST
    value: "redis://dragonflydb.dragonflydb.svc.cluster.local:6379"
  # Optional: Redis cluster mode (set to "true" if using Redis Cluster)
  # Dragonfly is single-node compatible, so leave as default (false)
  # - name: KUMOMTA_REDIS_CLUSTER_MODE
  #   value: "false"
  # Optional: Redis pool size (default: 100)
  # - name: KUMOMTA_REDIS_POOL_SIZE
  #   value: "100"
  # Optional: Read from replicas (default: true)
  # - name: KUMOMTA_REDIS_READ_FROM_REPLICAS
  #   value: "true"
  # Optional: Redis authentication (if enabled on Dragonfly)
  # - name: KUMOMTA_REDIS_USERNAME
  #   value: ""
  # - name: KUMOMTA_REDIS_PASSWORD
  #   value: ""
  - name: KUMOMTA_TRUSTED_HOSTS
    value: "127.0.0.1,0.0.0.0/0"  # localhost + toutes les IPs

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}
podLabels: {}

podSecurityContext:
  runAsUser: 999
  fsGroup: 999
  sysctls:
    - name: net.ipv4.ip_local_port_range
      value: "5000 63000"
  # fsGroup: 2000

securityContext:
  {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  type: ClusterIP
  httpPort: 8000
  metricsPort: 8080
  smtpPort: 2500

serviceMonitor:
  enabled: true
  # Port used for metrics endpoint (use port name "http" or port number)
  # Defaults to "http" (port name) which is more robust
  targetPort: "http"
  # Additional labels for ServiceMonitor (e.g., for Prometheus discovery)
  labels:
    release: prometheus-lab
  # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/basic-auth.md
  # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.BasicAuth
  # basicAuth: {}
  # basicAuth:
  #   username:
  #     name: foo # name of secret
  #     key: username # key in secret
  #   password:
  #     name: foo # name of secret
  #     key: username # key in secret

ingress:
  enabled: false
  className: "nginx-internal"
  annotations:
    {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources:
  {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

# livenessProbe:
# httpGet:
#   path: /
#   port: http
readinessProbe:
  httpGet:
    path: /api/check-liveness/v1
    port: http

lifecycleHook:
  preStop:
    exec:
      command:
        - /bin/sh
        - -c
        - sleep 15

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80
  # additionalMetrics: []

# Additional volumes on the output StatefulSet definition.
volumes:
  - name: kumo-configs
    configMap:
      name: kumo-configs
  - name: http-listener-keys
    secret:
      secretName: http-listener-keys
      optional: false
  - name: dkim-keys-talk-stir-com
    secret:
      secretName: dkim-keys-talk-stir-com
      optional: true
  - name: kumo-logs
    emptyDir: {}
  # Use emptyDir instead of PVC for spool storage (data is ephemeral)
  # Set to false to use PersistentVolumeClaim instead
  - name: spool
    emptyDir: {}

# Additional volumeMounts on the output StatefulSet definition.
volumeMounts:
  - name: kumo-configs
    mountPath: "/opt/kumomta/etc/policy"
    readOnly: true
  - name: http-listener-keys
    mountPath: "/opt/kumomta/etc/http_listener_keys/"
    readOnly: true
  - name: dkim-keys-talk-stir-com
    mountPath: "/opt/kumomta/etc/policy/dkim"
    readOnly: true
  - name: kumo-logs
    mountPath: "/var/log/kumomta"
  - name: spool
    mountPath: "/var/spool/kumomta"

# PersistentVolumeClaim templates - DISABLED by default
# Uncomment and configure to use persistent storage instead of emptyDir
# volumeClaimTemplates:
#   - metadata:
#       name: spool
#     spec:
#       accessModes: ["ReadWriteOnce"]
#       resources:
#         requests:
#           storage: 25Gi

tolerations: []

affinity: {}

topologySpreadConstraints: []

tsa:
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80
    # additionalMetrics: []
  env:
    # TSA Trusted Hosts: IPs/CIDRs allowed to access TSA HTTP endpoints without auth
    # For Kubernetes, allow connections from all pods in the cluster
    # Format: comma-separated list of IPs or CIDR blocks
    - name: KUMOMTA_TSA_TRUSTED_HOSTS
      value: "0.0.0.0/0,::/0"
  deployment:
    command:
      - /opt/kumomta/sbin/tsa-daemon
      - --policy
      - /opt/kumomta/etc/policy/tsa_init.lua
    # args: []
  # livenessProbe:
  # httpGet:
  #   path: /
  #   port: http
  readinessProbe:
    httpGet:
      path: /get_config_v1/shaping.toml
      port: http
  lifecycleHook:
    preStop:
      exec:
        command:
          - /bin/sh
          - -c
          - sleep 15
  podLabels: {}
  ingress:
    enabled: false
    className: "nginx-internal"
    annotations:
      {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    hosts:
      - host: chart-example.local
        paths:
          - path: /
            pathType: ImplementationSpecific
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local
    # Additional volumes on the TSA deployment definition.
  # PersistentVolumeClaim templates - DISABLED by default
  # Uncomment and configure to use persistent storage instead of emptyDir
  # volumeClaimTemplates:
  #   - metadata:
  #       name: tsa-spool
  #     spec:
  #       accessModes: ["ReadWriteOnce"]
  #       resources:
  #         requests:
  #           storage: 5Gi
  volumes:
    - name: kumo-configs
      configMap:
        name: kumo-configs
    - name: http-listener-keys
      secret:
        secretName: http-listener-keys
        optional: false
    - name: kumo-logs
      emptyDir: {}
    # Use emptyDir instead of PVC for TSA spool storage (data is ephemeral)
    - name: tsa-spool
      emptyDir: {}

  # Additional volumeMounts on the TSA deployment definition.
  volumeMounts:
    - name: kumo-configs
      mountPath: "/opt/kumomta/etc/policy"
      readOnly: true
    - name: http-listener-keys
      mountPath: "/opt/kumomta/etc/http_listener_keys/"
      readOnly: true
    - name: kumo-logs
      mountPath: "/var/log/kumo"
    - name: tsa-spool
      mountPath: "/var/spool/kumomta"
  service:
    type: ClusterIP
    httpPort: 8008

  tolerations: []

  affinity: {}

sink:
  enabled: true
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80
    # additionalMetrics: []
  deployment:
    command:
      - /opt/kumomta/sbin/kumod
      - --policy
      - /opt/kumomta/etc/policy/sink.lua
    # args: []
  # livenessProbe:
  # httpGet:
  #   path: /
  #   port: http
  # readinessProbe:
  lifecycleHook:
    preStop:
      exec:
        command:
          - /bin/sh
          - -c
          - sleep 15

  podLabels: {}

  service:
    type: ClusterIP
    httpPort: 8008
    smtpPort: 25

  ingress:
    enabled: false
    className: "nginx-internal"
    annotations:
      {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    hosts:
      - host: chart-example.local
        paths:
          - path: /
            pathType: ImplementationSpecific
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local
    # Additional volumes on the TSA deployment definition.
    #
  volumes:
    - name: sink-configs
      configMap:
        name: sink-configs
    - name: kumo-logs
      emptyDir: {}
  volumeMounts:
    - name: sink-configs
      mountPath: "/opt/kumomta/etc/policy"
      readOnly: true
    - name: kumo-logs
      mountPath: "/var/log/kumo"

# HTTP Listener Keys Secret configuration
# This secret stores username/password pairs for HTTP API authentication
# Each key in the secret is a username, and the value is the password (base64 encoded)
# 
# IMPORTANT: Sensitive values (passwords) should be defined in values.secrets.yaml
# Load secrets with: helm install/upgrade ... --values values.yaml --values values.secrets.yaml
# See values.secrets.yaml.example for the structure
httpListenerKeys:
  enabled: true
  secretName: http-listener-keys
  # Users configuration: username -> password (plain text, will be base64 encoded)
  # Generate passwords using: openssl rand -hex 16
  # NOTE: Define actual passwords in values.secrets.yaml (not committed to git)
  # Example structure (put actual values in values.secrets.yaml):
  # users:
  #   user1: "your-secure-password-here"
  #   admin: "another-secure-password"
  users: {}

# DKIM Keys Secret configuration (per domain/binding group)
# Each domain/binding group has its own secret configuration
# 
# IMPORTANT: Sensitive values (private keys) should be defined in values.secrets.yaml
# Load secrets with: helm install/upgrade ... --values values.yaml --values values.secrets.yaml
# See values.secrets.yaml.example for the structure
dkimKeys:
  # StirTalk binding group - talk.stir.com domain
  talkStirCom:
    enabled: true
    secretName: dkim-keys-talk-stir-com
    # Private key content (plain text, will be base64 encoded by Helm)
    # This key is loaded from configuration_momentum/global/dk/talk.stir.com/052024s2048.key
    # IMPORTANT: The key filename in the secret must match the filename specified in dkim_talk.stir.com.toml
    # NOTE: Define actual private key in values.secrets.yaml (not committed to git)
    privateKey: ""

## Debug pod with network tools (ping, curl, netcat, etc.)
## Useful for debugging network connectivity issues
debug:
  enabled: true
  image:
    repository: nicolaka/netshoot
    tag: latest
    pullPolicy: IfNotPresent
  resources:
    requests:
      memory: "64Mi"
      cpu: "100m"
    limits:
      memory: "128Mi"
      cpu: "200m"

## Extra manifests to deploy as an array
extraManifests:
  []
  # - apiVersion: v1
  #   kind: ConfigMap
  #   metadata:
  #     labels:
  #       name: prometheus-extra
  #   data:
  #     extra-data: "value"
